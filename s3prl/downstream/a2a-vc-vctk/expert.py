# -*- coding: utf-8 -*- #
"""*********************************************************************************************"""
#   FileName     [ expert.py ]
#   Synopsis     [ the any-to-any voice conversion downstream wrapper ]
#   Author       [ Wen-Chin Huang (https://github.com/unilight) ]
#   Copyright    [ Copyright(c), Toda Lab, Nagoya University, Japan ]
"""*********************************************************************************************"""


import os
import numpy as np
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from scipy.io.wavfile import write
from tqdm import tqdm
import yaml

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
from omegaconf import OmegaConf, MISSING

from .model import TacoVC, ConfTacoVC
from .data.datamodule import WavMelEmbVcData, ConfWavMelEmbVcData
from .data.dataset import Stat
from .utils import write_hdf5


@dataclass
class ConfCkptLog:
    dir_root: str = MISSING
    name_exp: str = MISSING
    name_version: str = MISSING

@dataclass
class ConfTrain:
    max_epochs: int = MISSING
    # max_steps: "${train_steps}"
    val_interval_epoch: int = MISSING
    # val_interval_step: 10000
    profiler: Optional[str] = MISSING
    ckpt_log: ConfCkptLog = ConfCkptLog()

@dataclass
class ConfDownstreamExpert:
    """
    Configuration of TacoVC trainer `DownstreamExpert`.

    Args:
        dim_unit - Dimension size of unit
        upstream_rate - Rate of upstream output [unit/sec]
        dim_mel - Feature dimension size of mel-spectrogram
        sr_for_mel - Sampling rate of waveform for mel-spectrogram
        mel_hop_length - STFT hop length of mel-spectrogram
        train_steps - The number of training steps
    """
    dim_unit: int = MISSING
    upstream_rate: int = MISSING
    dim_mel: int = MISSING
    sr_for_mel: int = MISSING
    mel_hop_length: int = MISSING
    train_steps: int = MISSING
    model: ConfTacoVC = ConfTacoVC()
    data: ConfWavMelEmbVcData = ConfWavMelEmbVcData()
    train: ConfTrain = ConfTrain()
    expdir: str = MISSING

class DownstreamExpert(nn.Module):
    """S3PRL interface of a2a-vc-vctk
    """

    def __init__(self,
        upstream_dim: int,
        upstream_rate: int,
        downstream_expert,
        **kwargs
    ):
        """
        Args:
            upstream_dim - Feature dimension size of upstream output
            upstream_rate - Rate of upstream output [unit/sec]
            downstream_expert - the `downstream_expert` attribute of config
        """
        super().__init__()

        # Config validation
        schema = OmegaConf.structured(ConfDownstreamExpert)
        raw_config = OmegaConf.create(downstream_expert)
        self._conf = OmegaConf.merge(schema, raw_config)

        # Upstream variable validations
        ## Unit dimension
        assert upstream_dim == self._conf.dim_unit, f"Unit dim {upstream_dim} != {self._conf.dim_unit}"
        ## Unit rate [unit/sec]
        assert upstream_rate == self._conf.upstream_rate, f"Unit rate {upstream_rate} != {self._conf.upstream_rate}"

        # Data setup
        self._data = WavMelEmbVcData(self._conf.data)
        self._data.prepare_data()
        self._data.setup()

        # Model instantiation
        self._model = TacoVC(
            conf=self._conf.model,
            stats=self._data.dataset_train.acquire_spec_stat()
        )

    def forward(self,
                split,
                input_features,
                acoustic_features,
                acoustic_features_padded,
                acoustic_feature_lengths,
                spk_embs,
                vc_ids,
                records,
                **kwargs):
        """(S3PRL API) Forward for train/val/test.

        Args:
            split: mode
            input_features: list of unpadded features generated by the upstream
            acoustic_features: List[Tensor(`lmspc`)], not used...?
            acoustic_features_padded: `acoustic_features` padded by PyTorch function
            acoustic_feature_lengths: Tensor(feature time length)
            spk_embs: Tensor(`ref_spk_emb`)
            vc_ids: List[(target_spk, source_spk, uttr_name)]
            records: Result container
        """

        device = input_features[0].device
        spk_embs = spk_embs.to(device)
        acoustic_features_padded = acoustic_features_padded.to(device)

        # input_feature_lengths::(B, T)
        input_feature_lengths = torch.IntTensor([feature.shape[0] for feature in input_features])
        # (T, Feat)[] -> (B, Tmax, Feat)
        input_features = pad_sequence(input_features, batch_first=True).to(device=device)

        if split == "train":
            results = self._model.training_step((
                input_features, input_feature_lengths, \
                acoustic_features_padded, acoustic_feature_lengths, \
                spk_embs, device,
                ), 1
            )
            loss = results["loss"]        
        else: # eval | test
            results = self._model.validation_step((
                input_features, input_feature_lengths, \
                acoustic_features_padded, acoustic_feature_lengths, \
                spk_embs, device, records
                ), 2
            )
            loss = results["val_loss"]
            records["vc_ids"] += vc_ids

        records['loss'].append(loss.item())
        return loss

    def log_records(self, split, records, logger, global_step, batch_ids, total_batch_num, **kwargs):
        """(S3PRL API) Logging.

        Report loss, save features (and generated-waveform).

        Args:
            split: "train" | "dev" | "test"
            records: Logging target record
                loss
                feature_lengths
                predicted_features
                vc_ids
            logger: (maybe) TensorBoard logger
            global_step: Number of global step, used for file name and TB logging
            batch_ids: Not Used
            total_batch_num: Not Used
            kwargs: Not Used
        Returns:
            empty array
        """

        # Loss logging in console and TB
        loss = torch.FloatTensor(records['loss']).mean().item()
        print(f'{split} loss: {loss:.6f}')
        logger.add_scalar(f'example/{split}', loss, global_step=global_step)

        # Generate waveform w/ Griffin-Lim and save it
        if split in ["dev", "test"]:
            # Path preparation
            cnf = self._conf.train.ckpt_log
            exp_dir = Path(cnf.dir_root) / cnf.name_exp / cnf.name_version
            root = exp_dir / str(global_step) / split
            hdf5_save_dir = root / "hdf5"
            wav_save_dir = root  / "wav"
            hdf5_save_dir.mkdir(exist_ok=True, parents=True)
            wav_save_dir.mkdir(exist_ok=True, parents=True)

            for i, (tgt_spk, src_spk, uttr_name) in enumerate(tqdm(
                records["vc_ids"],
                dynamic_ncols=True, desc="Inference/Generate_waveform"
            )):
                # Save each item in `predicted_features` w/o contents modification
                # No.i in a batch
                length = int(records["feature_lengths"][i])
                # Remove padding:: (T_mel, Freq)
                fbank = np.array(records["predicted_features"][i])[:length]

                # Path preparation
                file_stem = f"{tgt_spk}_from_{src_spk}_{uttr_name}"
                hdf5_save_path = hdf5_save_dir / f"{file_stem}.h5"
                wav_save_path = wav_save_dir / f"{file_stem}.wav"

                # save generated features into hdf5 files
                write_hdf5(hdf5_save_path, "feats", fbank)

                # Waveform generation from feature for reporting
                # wav = vocoder(fbank)
                ## save
                # write(
                #     wav_save_path,
                #     <sampling_rate>,
                #     (y * np.iinfo(np.int16).max).astype(np.int16),
                # )
        return []

    def get_dataloader(self, split: str) -> DataLoader:
        """(S3PRL API) Generate data loader."""
        if split == "train":
            return self._data.train_dataloader()
        elif split == "dev":
            return self._data.val_dataloader()
        elif split == "test":
            return self._data.test_dataloader()
